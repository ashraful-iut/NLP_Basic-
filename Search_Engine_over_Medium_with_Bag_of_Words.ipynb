{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashraful-iut/NLP_Basic-/blob/main/Search_Engine_over_Medium_with_Bag_of_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Notebook of the course [Pratical NLP with Python](https://www.nlplanet.org/course-practical-nlp/).\n",
        "\n",
        "Lesson: [Project: Search Engine over Medium with Bag of Words](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/07-search-engine-bow.html)\n",
        "\n",
        "Made by: [Fabio Chiusano](https://www.linkedin.com/in/fabio-chiusano-b6a3b311b/)\n",
        "\n",
        "Table of Contents:\n",
        "- Lesson Code\n",
        "  - Libraries\n",
        "  - Download the Dataset\n",
        "  - Data Preprocessing\n",
        "  - Make Queries\n",
        "  - Removing Stopwords\n",
        "- Code Exercises\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "T-tPDPDD74UW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson Code"
      ],
      "metadata": {
        "id": "OplE039g7_It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "ZfcavXUU2W1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "AA1xv7mU0jLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_QfzwGrzttk"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Dataset"
      ],
      "metadata": {
        "id": "OomcSEf_8Bpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset of Medium articles from \n",
        "# https://huggingface.co/datasets/fabiochiu/medium-articles\n",
        "df_articles = pd.read_csv(\n",
        "  hf_hub_download(\"fabiochiu/medium-articles\", repo_type=\"dataset\",\n",
        "                  filename=\"medium_articles.csv\")\n",
        ")\n",
        "\n",
        "# There are 192,368 articles in total, but let's keep only the first 10,000 to\n",
        "# make computations faster\n",
        "df_articles = df_articles.sample(n=10000)\n",
        "\n",
        "df_articles.head()"
      ],
      "metadata": {
        "id": "xqYC9pCa0byG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "s7lhbQA78ndp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of occurrences of each token in each text\n",
        "texts_lowercase = df_articles[\"text\"].str.lower()\n",
        "texts_lowercase_tokenized = texts_lowercase.apply(word_tokenize)\n",
        "token_counters = texts_lowercase_tokenized.apply(Counter).values.tolist()\n",
        "\n",
        "# show the tokens found in the first article with at least 10 occurrences\n",
        "print({token: n_occ for token, n_occ in token_counters[0].items() if n_occ >= 10})"
      ],
      "metadata": {
        "id": "R0DYYe-O2CDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Queries"
      ],
      "metadata": {
        "id": "WA8saqSu8Hu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the query\n",
        "query = \"data science nlp\"\n",
        "query_tokens = word_tokenize(query)"
      ],
      "metadata": {
        "id": "Ki7OUT6x6QsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute a matching score for each text with respect to the query. The score is\n",
        "# the number of times each token in the query can be found in a specific text.\n",
        "def get_scores(query_tokens, token_counters):\n",
        "  scores = []\n",
        "  for token_counter in token_counters:\n",
        "    matches = [token_counter[query_token] for query_token in query_tokens]\n",
        "    total_score = sum(matches)\n",
        "    scores.append(total_score)\n",
        "  return scores\n",
        "\n",
        "scores = get_scores(query_tokens, token_counters)"
      ],
      "metadata": {
        "id": "luxQZ0GV6bG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the top_n articles with the highest scores and show them\n",
        "def show_best_results(df_articles, scores, top_n=10):\n",
        "  best_indexes = np.argsort(scores)[::-1]\n",
        "  for position, idx in enumerate(best_indexes[:top_n]):\n",
        "    row = df_articles.iloc[idx]\n",
        "    title = row[\"title\"]\n",
        "    score = scores[idx]\n",
        "    print(f\"{position + 1} [score = {score}]: {title}\")\n",
        "\n",
        "show_best_results(df_articles, scores)"
      ],
      "metadata": {
        "id": "ueACDixO7Nqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try a different query\n",
        "query = \"how to learn data science\"\n",
        "query_tokens = word_tokenize(query)\n",
        "scores = get_scores(query_tokens, token_counters)\n",
        "show_best_results(df_articles, scores)"
      ],
      "metadata": {
        "id": "XJb3zede-ALy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Stopwords"
      ],
      "metadata": {
        "id": "svWIo5xt-hgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "english_stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "id": "0q8Lhq7Z-zF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.punctuation)"
      ],
      "metadata": {
        "id": "etN4VfzWlolj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of occurrences of each token in each text\n",
        "texts_lowercase = df_articles[\"text\"].str.lower()\n",
        "texts_lowercase_tokenized = texts_lowercase.apply(word_tokenize)\n",
        "texts_lowercase_tokenized_no_sw = texts_lowercase_tokenized.apply(\n",
        "  lambda token_list: [token for token in token_list\n",
        "                      if token not in english_stopwords and\n",
        "                      token not in string.punctuation]\n",
        ")\n",
        "token_counters = texts_lowercase_tokenized_no_sw.apply(Counter).values.tolist()\n",
        "\n",
        "# show the tokens found in the first article with at least 6 occurrences\n",
        "print({token: n_occ for token, n_occ in token_counters[0].items() if n_occ >= 6})"
      ],
      "metadata": {
        "id": "yulrfV6z-U22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the query and remove stopwords\n",
        "query = \"how to learn data science\"\n",
        "query_tokens = word_tokenize(query)\n",
        "query_tokens_no_sw = [token for token in query_tokens\n",
        "                      if token not in english_stopwords and\n",
        "                      token not in string.punctuation]\n",
        "print(f\"Tokenized query without stopwords: {query_tokens_no_sw}\")\n",
        "print()\n",
        "\n",
        "# show best results\n",
        "scores = get_scores(query_tokens, token_counters)\n",
        "show_best_results(df_articles, scores)"
      ],
      "metadata": {
        "id": "SRkek2iQ_GpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Exercises"
      ],
      "metadata": {
        "id": "4G76rXot8N_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Reimplement the search engine logic using the `CountVectorizer` class from `sklearn`"
      ],
      "metadata": {
        "id": "Ba4i9rydFv9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE CODE HERE"
      ],
      "metadata": {
        "id": "u2TztyoV8P_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Reimplement the search engine logic using the `CountVectorizer` class from `sklearn`. Fix the `max_df` parameter of the `CountVectorizer` so that the query \"how to learn data science\" returns good results even without manually removing stopwords"
      ],
      "metadata": {
        "id": "IK7FAWDVGsir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE CODE HERE"
      ],
      "metadata": {
        "id": "witxXGo-G3DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Write a new scoring function for the search engine that counts the number of tokens in the query that have at least one occurrence in the document (instead of summing all the occurrences in the document). Test the new scoring function with some queries"
      ],
      "metadata": {
        "id": "lzFAAwL_F-Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE CODE HERE"
      ],
      "metadata": {
        "id": "JB4rQf0DGb4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}